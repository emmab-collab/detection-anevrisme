{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation - CTA/MRA/MRI\n",
    "\n",
    "Ce notebook crée les datasets de cubes 3D pour l'entraînement du modèle.\n",
    "\n",
    "**Étapes** :\n",
    "1. Chargement des données localisateurs\n",
    "2. Filtrage par modalité (CTA, MRA, MRI T1post, MRI T2)\n",
    "3. Vérification de la disponibilité des données\n",
    "4. Test sur un exemple\n",
    "5. Extraction de cubes positifs (contenant anévrisme) et négatifs\n",
    "6. Sauvegarde au format .npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import du package src\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src import (\n",
    "    SERIES_DIR,\n",
    "    TRAIN_CSV,\n",
    "    TRAIN_LOCALIZERS_CSV,\n",
    "    PROCESSED_DIR,\n",
    "    print_config\n",
    ")\n",
    "from src.data import ajouter_Modality\n",
    "from src.preprocessing import preprocessing_volume_and_coords\n",
    "from src.visualization import show_middle_slices, show_slice_with_point\n",
    "from src.bricks import Preprocessor, DatasetBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration et chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la configuration automatique\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement DataFrames (chemins automatiques)\n",
    "df_loc = pd.read_csv(TRAIN_LOCALIZERS_CSV)\n",
    "df_train = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# Ajout modalité\n",
    "df_loc = ajouter_Modality(df_loc, df_train)\n",
    "\n",
    "print(f\"Total localizers: {len(df_loc)}\")\n",
    "print(f\"\\nModalités:\")\n",
    "print(df_loc['Modality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filtrage par modalité\n",
    "\n",
    "Choisissez la modalité à traiter (CTA, MRA, MRI T1post, MRI T2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection modalité\n",
    "MODALITY = \"CTA\"  # Changez selon besoin: \"CTA\", \"MRA\", \"MRI T1post\", \"MRI T2\"\n",
    "\n",
    "df_modality = df_loc[df_loc['Modality'] == MODALITY].reset_index(drop=True)\n",
    "print(f\"Nombre de {MODALITY}: {len(df_modality)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vérification de la disponibilité des données\n",
    "\n",
    "Avant de traiter les données, vérifions quels patients sont réellement disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier combien de patients de la modalité sont disponibles localement\n",
    "available_patients = []\n",
    "for i in range(len(df_modality)):\n",
    "    series_uid = df_modality.iloc[i]['SeriesInstanceUID']\n",
    "    patient_path = os.path.join(SERIES_DIR, series_uid)\n",
    "    if os.path.exists(patient_path):\n",
    "        available_patients.append(series_uid)\n",
    "\n",
    "print(f\"Patients {MODALITY} dans le CSV: {len(df_modality)}\")\n",
    "print(f\"Patients {MODALITY} disponibles localement: {len(available_patients)}\")\n",
    "\n",
    "if len(available_patients) > 0:\n",
    "    print(f\"\\n✓ {len(available_patients)} patients prêts pour le traitement\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Aucun patient disponible localement\")\n",
    "    print(f\"Les données DICOM doivent être téléchargées dans: {SERIES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test sur un exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sur le premier patient disponible\n",
    "if len(available_patients) > 0:\n",
    "    # Utiliser le premier patient disponible identifié à l'étape 3\n",
    "    series_uid = available_patients[0]\n",
    "    patient_path = os.path.join(SERIES_DIR, series_uid)\n",
    "    \n",
    "    print(f\"Patient sélectionné: {series_uid}\")\n",
    "    \n",
    "    try:\n",
    "        volume, aneurysm_coords = preprocessing_volume_and_coords(\n",
    "            SERIES_DIR, patient_path, df_modality\n",
    "        )\n",
    "        \n",
    "        print(f\"Volume shape: {volume.shape}\")\n",
    "        print(f\"Aneurysm coordinates: {aneurysm_coords}\")\n",
    "        \n",
    "        # Visualisation\n",
    "        show_middle_slices(volume)\n",
    "        show_slice_with_point(volume, aneurysm_coords, plane=\"axial\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(f\"⚠️ Aucun patient {MODALITY} disponible localement\")\n",
    "    print(f\"Exécutez d'abord l'étape 3 pour vérifier les données disponibles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Création des datasets par modalité\n",
    "\n",
    "Cette section utilise la classe `DatasetBuilder` pour créer un dataset par modalité :\n",
    "- **CTA** : Angiographie par tomodensitométrie\n",
    "- **MRA** : Angiographie par résonance magnétique\n",
    "- **MRI T1post** : IRM T1 avec contraste\n",
    "- **MRI T2** : IRM T2\n",
    "\n",
    "Chaque dataset contient :\n",
    "- Des cubes positifs (contenant des anévrismes) \n",
    "- Des cubes négatifs (sans anévrisme)\n",
    "- Des vecteurs de position (one-hot encoding)\n",
    "- Les labels et patient IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le preprocessor et le dataset builder\n",
    "preprocessor = Preprocessor()\n",
    "builder = DatasetBuilder(\n",
    "    preprocessor=preprocessor,\n",
    "    cube_size=48,\n",
    "    series_dir=SERIES_DIR\n",
    ")\n",
    "\n",
    "print(f\"DatasetBuilder initialisé: {builder}\")\n",
    "print(f\"\\nPrêt à construire les datasets pour toutes les modalités\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Sauvegarde des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde de tous les datasets créés\n",
    "# Décommenter après la création des datasets\n",
    "\n",
    "# if 'datasets' in locals() and len(datasets) > 0:\n",
    "#     print(\"Sauvegarde des datasets...\\n\")\n",
    "#     \n",
    "#     for modality, dataset in datasets.items():\n",
    "#         # Nom de fichier standardisé\n",
    "#         filename = f\"{modality.lower().replace(' ', '_')}_dataset.npz\"\n",
    "#         output_path = os.path.join(PROCESSED_DIR, filename)\n",
    "#         \n",
    "#         # Sauvegarder\n",
    "#         builder.save(dataset, output_path)\n",
    "#         \n",
    "#         # Statistiques\n",
    "#         print(f\"\\n{modality} Dataset:\")\n",
    "#         print(f\"  Fichier: {filename}\")\n",
    "#         print(f\"  Total cubes: {len(dataset['cubes'])}\")\n",
    "#         print(f\"  Positifs: {dataset['labels'].sum():.0f}\")\n",
    "#         print(f\"  Négatifs: {(1 - dataset['labels']).sum():.0f}\")\n",
    "#         print(f\"  Balance: {dataset['labels'].mean():.2%} positive\")\n",
    "#     \n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(f\"✓ {len(datasets)} datasets sauvegardés dans {PROCESSED_DIR}\")\n",
    "#     print(f\"{'='*70}\")\n",
    "# else:\n",
    "#     print(\"⚠️ Aucun dataset à sauvegarder.\")\n",
    "#     print(\"Exécutez d'abord la cellule de création des datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Résumé des datasets créés\n",
    "\n",
    "Visualisation finale des datasets créés et leurs caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résumé final des datasets créés\n",
    "# Décommenter pour afficher le résumé\n",
    "\n",
    "# if 'datasets' in locals() and len(datasets) > 0:\n",
    "#     import pandas as pd\n",
    "#     \n",
    "#     # Créer un tableau récapitulatif\n",
    "#     summary_data = []\n",
    "#     for modality, dataset in datasets.items():\n",
    "#         summary_data.append({\n",
    "#             'Modalité': modality,\n",
    "#             'Total Cubes': len(dataset['cubes']),\n",
    "#             'Positifs': int(dataset['labels'].sum()),\n",
    "#             'Négatifs': int((1 - dataset['labels']).sum()),\n",
    "#             'Balance (%)': f\"{dataset['labels'].mean()*100:.1f}%\",\n",
    "#             'Fichier': f\"{modality.lower().replace(' ', '_')}_dataset.npz\"\n",
    "#         })\n",
    "#     \n",
    "#     df_summary = pd.DataFrame(summary_data)\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"RÉSUMÉ DES DATASETS CRÉÉS\")\n",
    "#     print(\"=\"*80 + \"\\n\")\n",
    "#     print(df_summary.to_string(index=False))\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(f\"Total: {len(datasets)} datasets créés\")\n",
    "#     print(f\"Localisation: {PROCESSED_DIR}\")\n",
    "#     print(\"=\"*80)\n",
    "# else:\n",
    "#     print(\"Aucun dataset créé pour le moment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du dataset (décommenter après la création)\n",
    "# if 'dataset' in locals():\n",
    "#     output_path = os.path.join(PROCESSED_DIR, f\"{MODALITY}_dataset.npz\")\n",
    "#     builder.save(dataset, output_path)\n",
    "#     \n",
    "#     # Afficher les statistiques du dataset\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Dataset Statistics:\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     print(f\"Total cubes: {len(dataset['cubes'])}\")\n",
    "#     print(f\"Cube shape: {dataset['cubes'][0].shape}\")\n",
    "#     print(f\"Positive samples: {dataset['labels'].sum():.0f}\")\n",
    "#     print(f\"Negative samples: {(1 - dataset['labels']).sum():.0f}\")\n",
    "#     print(f\"Balance: {dataset['labels'].mean():.2%} positive\")\n",
    "# else:\n",
    "#     print(\"⚠️ Aucun dataset à sauvegarder. Exécutez d'abord la cellule de création.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
