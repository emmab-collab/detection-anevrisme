{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation - CTA/MRA/MRI\n",
    "\n",
    "Ce notebook cr√©e les datasets de cubes 3D pour l'entra√Ænement du mod√®le.\n",
    "\n",
    "> **üìå Note importante** : Ce notebook utilise un **√©chantillon de 20 s√©ries DICOM** pour d√©monstration.\n",
    "> \n",
    "> Le projet original sur Kaggle a trait√© **4000+ s√©ries** avec la m√™me architecture.\n",
    "> Cette version permet de visualiser la structure des datasets et de tester le code localement.\n",
    "\n",
    "**√âtapes** :\n",
    "1. Chargement des donn√©es localisateurs\n",
    "2. Filtrage par modalit√© (CTA, MRA, MRI T1post, MRI T2)\n",
    "3. V√©rification de la disponibilit√© des donn√©es\n",
    "4. Test sur un exemple\n",
    "5. Extraction de cubes positifs (contenant an√©vrisme) et n√©gatifs\n",
    "6. Sauvegarde au format .npz\n",
    "7. R√©sum√© des datasets cr√©√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import du package src\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src import (\n",
    "    SERIES_DIR,\n",
    "    TRAIN_CSV,\n",
    "    TRAIN_LOCALIZERS_CSV,\n",
    "    PROCESSED_DIR,\n",
    "    print_config\n",
    ")\n",
    "from src.data import ajouter_Modality\n",
    "from src.preprocessing import preprocessing_volume_and_coords\n",
    "from src.visualization import show_middle_slices, show_slice_with_point\n",
    "from src.bricks import Preprocessor, DatasetBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration et chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la configuration automatique\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement DataFrames (chemins automatiques)\n",
    "df_loc = pd.read_csv(TRAIN_LOCALIZERS_CSV)\n",
    "df_train = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# Ajout modalit√©\n",
    "df_loc = ajouter_Modality(df_loc, df_train)\n",
    "\n",
    "print(f\"Total localizers: {len(df_loc)}\")\n",
    "print(f\"\\nModalit√©s:\")\n",
    "print(df_loc['Modality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filtrage par modalit√©\n",
    "\n",
    "Choisissez la modalit√© √† traiter (CTA, MRA, MRI T1post, MRI T2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lection modalit√©\n",
    "MODALITY = \"CTA\"  # Changez selon besoin: \"CTA\", \"MRA\", \"MRI T1post\", \"MRI T2\"\n",
    "\n",
    "df_modality = df_loc[df_loc['Modality'] == MODALITY].reset_index(drop=True)\n",
    "print(f\"Nombre de {MODALITY}: {len(df_modality)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. V√©rification de la disponibilit√© des donn√©es\n",
    "\n",
    "Avant de traiter les donn√©es, v√©rifions quels patients sont r√©ellement disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier combien de patients de la modalit√© sont disponibles localement\n",
    "available_patients = []\n",
    "for i in range(len(df_modality)):\n",
    "    series_uid = df_modality.iloc[i]['SeriesInstanceUID']\n",
    "    patient_path = os.path.join(SERIES_DIR, series_uid)\n",
    "    if os.path.exists(patient_path):\n",
    "        available_patients.append(series_uid)\n",
    "\n",
    "print(f\"Patients {MODALITY} dans le CSV: {len(df_modality)}\")\n",
    "print(f\"Patients {MODALITY} disponibles localement: {len(available_patients)}\")\n",
    "\n",
    "if len(available_patients) > 0:\n",
    "    print(f\"\\n‚úì {len(available_patients)} patients pr√™ts pour le traitement\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Aucun patient disponible localement\")\n",
    "    print(f\"Les donn√©es DICOM doivent √™tre t√©l√©charg√©es dans: {SERIES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test sur un exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sur le premier patient disponible\n",
    "if len(available_patients) > 0:\n",
    "    # Utiliser le premier patient disponible identifi√© √† l'√©tape 3\n",
    "    series_uid = available_patients[0]\n",
    "    patient_path = os.path.join(SERIES_DIR, series_uid)\n",
    "    \n",
    "    print(f\"Patient s√©lectionn√©: {series_uid}\")\n",
    "    \n",
    "    try:\n",
    "        volume, aneurysm_coords = preprocessing_volume_and_coords(\n",
    "            SERIES_DIR, patient_path, df_modality\n",
    "        )\n",
    "        \n",
    "        print(f\"Volume shape: {volume.shape}\")\n",
    "        print(f\"Aneurysm coordinates: {aneurysm_coords}\")\n",
    "        \n",
    "        # Visualisation\n",
    "        show_middle_slices(volume)\n",
    "        show_slice_with_point(volume, aneurysm_coords, plane=\"axial\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Aucun patient {MODALITY} disponible localement\")\n",
    "    print(f\"Ex√©cutez d'abord l'√©tape 3 pour v√©rifier les donn√©es disponibles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cr√©ation des datasets par modalit√©\n",
    "\n",
    "Cette section utilise la classe `DatasetBuilder` pour cr√©er un dataset par modalit√© :\n",
    "- **CTA** : Angiographie par tomodensitom√©trie\n",
    "- **MRA** : Angiographie par r√©sonance magn√©tique\n",
    "- **MRI T1post** : IRM T1 avec contraste\n",
    "- **MRI T2** : IRM T2\n",
    "\n",
    "Chaque dataset contient :\n",
    "- Des cubes positifs (contenant des an√©vrismes) \n",
    "- Des cubes n√©gatifs (sans an√©vrisme)\n",
    "- Des vecteurs de position (one-hot encoding)\n",
    "- Les labels et patient IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le preprocessor et le dataset builder\n",
    "preprocessor = Preprocessor()\n",
    "builder = DatasetBuilder(\n",
    "    preprocessor=preprocessor,\n",
    "    cube_size=48,\n",
    "    series_dir=SERIES_DIR\n",
    ")\n",
    "\n",
    "print(f\"DatasetBuilder initialis√©: {builder}\")\n",
    "print(f\"\\nPr√™t √† construire les datasets pour toutes les modalit√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Sauvegarde des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde de tous les datasets cr√©√©s\n",
    "# D√©commenter apr√®s la cr√©ation des datasets\n",
    "\n",
    "# if 'datasets' in locals() and len(datasets) > 0:\n",
    "#     print(\"Sauvegarde des datasets...\\n\")\n",
    "#     \n",
    "#     for modality, dataset in datasets.items():\n",
    "#         # Nom de fichier standardis√©\n",
    "#         filename = f\"{modality.lower().replace(' ', '_')}_dataset.npz\"\n",
    "#         output_path = os.path.join(PROCESSED_DIR, filename)\n",
    "#         \n",
    "#         # Sauvegarder\n",
    "#         builder.save(dataset, output_path)\n",
    "#         \n",
    "#         # Statistiques\n",
    "#         print(f\"\\n{modality} Dataset:\")\n",
    "#         print(f\"  Fichier: {filename}\")\n",
    "#         print(f\"  Total cubes: {len(dataset['cubes'])}\")\n",
    "#         print(f\"  Positifs: {dataset['labels'].sum():.0f}\")\n",
    "#         print(f\"  N√©gatifs: {(1 - dataset['labels']).sum():.0f}\")\n",
    "#         print(f\"  Balance: {dataset['labels'].mean():.2%} positive\")\n",
    "#     \n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(f\"‚úì {len(datasets)} datasets sauvegard√©s dans {PROCESSED_DIR}\")\n",
    "#     print(f\"{'='*70}\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è Aucun dataset √† sauvegarder.\")\n",
    "#     print(\"Ex√©cutez d'abord la cellule de cr√©ation des datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. R√©sum√© des datasets cr√©√©s\n",
    "\n",
    "Visualisation finale des datasets cr√©√©s et leurs caract√©ristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© final des datasets cr√©√©s\n",
    "# D√©commenter pour afficher le r√©sum√©\n",
    "\n",
    "# if 'datasets' in locals() and len(datasets) > 0:\n",
    "#     import pandas as pd\n",
    "#     \n",
    "#     # Cr√©er un tableau r√©capitulatif\n",
    "#     summary_data = []\n",
    "#     for modality, dataset in datasets.items():\n",
    "#         summary_data.append({\n",
    "#             'Modalit√©': modality,\n",
    "#             'Total Cubes': len(dataset['cubes']),\n",
    "#             'Positifs': int(dataset['labels'].sum()),\n",
    "#             'N√©gatifs': int((1 - dataset['labels']).sum()),\n",
    "#             'Balance (%)': f\"{dataset['labels'].mean()*100:.1f}%\",\n",
    "#             'Fichier': f\"{modality.lower().replace(' ', '_')}_dataset.npz\"\n",
    "#         })\n",
    "#     \n",
    "#     df_summary = pd.DataFrame(summary_data)\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"R√âSUM√â DES DATASETS CR√â√âS\")\n",
    "#     print(\"=\"*80 + \"\\n\")\n",
    "#     print(df_summary.to_string(index=False))\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(f\"Total: {len(datasets)} datasets cr√©√©s\")\n",
    "#     print(f\"Localisation: {PROCESSED_DIR}\")\n",
    "#     print(\"=\"*80)\n",
    "# else:\n",
    "#     print(\"Aucun dataset cr√©√© pour le moment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du dataset (d√©commenter apr√®s la cr√©ation)\n",
    "# if 'dataset' in locals():\n",
    "#     output_path = os.path.join(PROCESSED_DIR, f\"{MODALITY}_dataset.npz\")\n",
    "#     builder.save(dataset, output_path)\n",
    "#     \n",
    "#     # Afficher les statistiques du dataset\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Dataset Statistics:\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     print(f\"Total cubes: {len(dataset['cubes'])}\")\n",
    "#     print(f\"Cube shape: {dataset['cubes'][0].shape}\")\n",
    "#     print(f\"Positive samples: {dataset['labels'].sum():.0f}\")\n",
    "#     print(f\"Negative samples: {(1 - dataset['labels']).sum():.0f}\")\n",
    "#     print(f\"Balance: {dataset['labels'].mean():.2%} positive\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è Aucun dataset √† sauvegarder. Ex√©cutez d'abord la cellule de cr√©ation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
