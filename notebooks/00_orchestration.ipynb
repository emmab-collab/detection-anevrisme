{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline d'Orchestration - D√©tection d'An√©vrismes\n",
    "\n",
    "Ce notebook orchestre l'ensemble du pipeline de d√©tection d'an√©vrismes :\n",
    "1. Analyse exploratoire des donn√©es (EDA)\n",
    "2. Cr√©ation du dataset\n",
    "3. Augmentation de donn√©es\n",
    "4. Entra√Ænement du mod√®le\n",
    "5. √âvaluation\n",
    "\n",
    "**Avantages de cette approche** :\n",
    "- Pipeline centralis√© et reproductible\n",
    "- Classes modulaires et r√©utilisables\n",
    "- Facile √† d√©ployer en production\n",
    "- Testable et maintenable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Import des paths et config\n",
    "from src import (\n",
    "    SERIES_DIR, TRAIN_CSV, TRAIN_LOCALIZERS_CSV,\n",
    "    PROCESSED_DIR, MODELS_DIR, CHECKPOINTS_DIR,\n",
    "    print_config\n",
    ")\n",
    "\n",
    "# Import des bricks (composants de pipeline)\n",
    "from src.bricks import (\n",
    "    Preprocessor,\n",
    "    DatasetBuilder,\n",
    "    Augmentor,\n",
    "    EDA,\n",
    "    Trainer,\n",
    "    Predictor\n",
    ")\n",
    "\n",
    "# Import du mod√®le\n",
    "from src.models import UNet3DClassifier\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher la configuration des chemins\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres du pipeline\n",
    "MODALITY = \"CTA\"  # Modalit√© √† traiter\n",
    "CUBE_SIZE = 48\n",
    "N_AUGMENTATIONS = 12\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Modalit√©: {MODALITY}\")\n",
    "print(f\"Cube size: {CUBE_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Phase 1 : Analyse Exploratoire (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es\n",
    "df_train = pd.read_csv(TRAIN_CSV)\n",
    "df_localizers = pd.read_csv(TRAIN_LOCALIZERS_CSV)\n",
    "\n",
    "print(f\"S√©ries totales: {len(df_train)}\")\n",
    "print(f\"Localisateurs: {len(df_localizers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er l'analyseur EDA\n",
    "eda = EDA(df_train, df_localizers, SERIES_DIR)\n",
    "\n",
    "# G√©n√©rer le rapport complet\n",
    "eda.generate_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les distributions\n",
    "eda.plot_aneurysm_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Phase 2 : Cr√©ation du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le preprocessor\n",
    "preprocessor = Preprocessor(\n",
    "    target_spacing=(0.4, 0.4, 0.4),\n",
    "    crop_threshold=0.1\n",
    ")\n",
    "\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le dataset builder\n",
    "dataset_builder = DatasetBuilder(\n",
    "    preprocessor=preprocessor,\n",
    "    cube_size=CUBE_SIZE,\n",
    "    series_dir=SERIES_DIR\n",
    ")\n",
    "\n",
    "print(dataset_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construire le dataset pour la modalit√© choisie\n",
    "dataset = dataset_builder.build_dataset(\n",
    "    df_train=df_train,\n",
    "    df_localizers=df_localizers,\n",
    "    modality=MODALITY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le dataset\n",
    "dataset_path = os.path.join(PROCESSED_DIR, f\"{MODALITY}_dataset.npz\")\n",
    "dataset_builder.save(dataset, dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Phase 3 : Augmentation de Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er l'augmentor\n",
    "augmentor = Augmentor(\n",
    "    n_augmentations=N_AUGMENTATIONS,\n",
    "    grid_size=3,\n",
    "    max_displacement=3.0\n",
    ")\n",
    "\n",
    "print(augmentor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmenter le dataset (seulement les positifs)\n",
    "dataset_augmented = augmentor.augment_dataset(\n",
    "    dataset,\n",
    "    augment_negatives=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le dataset augment√©\n",
    "augmented_path = os.path.join(PROCESSED_DIR, f\"{MODALITY}_dataset_augmented.npz\")\n",
    "augmentor.save(dataset_augmented, augmented_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Phase 4 : Pr√©paration pour l'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un PyTorch Dataset\n",
    "class CubesDataset(Dataset):\n",
    "    def __init__(self, dataset_dict):\n",
    "        self.cubes = dataset_dict['cubes']\n",
    "        self.labels = dataset_dict['labels']\n",
    "        self.positions = dataset_dict['positions']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cubes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cube = torch.tensor(self.cubes[idx], dtype=torch.float32).unsqueeze(0)\n",
    "        label = self.labels[idx]\n",
    "        position = self.positions[idx]\n",
    "        \n",
    "        # Concat√©ner position (13) et label (1) ‚Üí (14,)\n",
    "        y = np.concatenate([position, [label]], axis=0)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        return cube, y\n",
    "\n",
    "print(\"‚úÖ Dataset class d√©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/val (80/20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_samples = len(dataset_augmented['cubes'])\n",
    "indices = np.arange(n_samples)\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=dataset_augmented['labels']\n",
    ")\n",
    "\n",
    "# Cr√©er les sous-datasets\n",
    "train_data = {\n",
    "    'cubes': dataset_augmented['cubes'][train_idx],\n",
    "    'labels': dataset_augmented['labels'][train_idx],\n",
    "    'positions': dataset_augmented['positions'][train_idx]\n",
    "}\n",
    "\n",
    "val_data = {\n",
    "    'cubes': dataset_augmented['cubes'][val_idx],\n",
    "    'labels': dataset_augmented['labels'][val_idx],\n",
    "    'positions': dataset_augmented['positions'][val_idx]\n",
    "}\n",
    "\n",
    "print(f\"Train: {len(train_idx)} cubes\")\n",
    "print(f\"Val: {len(val_idx)} cubes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les DataLoaders\n",
    "train_dataset = CubesDataset(train_data)\n",
    "val_dataset = CubesDataset(val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Phase 5 : Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le mod√®le\n",
    "model = UNet3DClassifier(in_ch=1, base_ch=32)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss combin√©e\n",
    "def combined_loss(pred, target, alpha=0.1):\n",
    "    \"\"\"Loss combin√©e: BCE pour positions + BCE pour label\"\"\"\n",
    "    pos_pred = torch.sigmoid(pred[:, :13])\n",
    "    pos_target = target[:, :13]\n",
    "    label_pred = pred[:, 13:]\n",
    "    label_target = target[:, 13:]\n",
    "    \n",
    "    loss_pos = nn.functional.binary_cross_entropy(pos_pred, pos_target)\n",
    "    loss_label = nn.functional.binary_cross_entropy_with_logits(label_pred, label_target)\n",
    "    \n",
    "    return alpha * loss_pos + loss_label\n",
    "\n",
    "# Optimiseur\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"‚úÖ Loss et optimiseur d√©finis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    criterion=combined_loss,\n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    checkpoint_dir=CHECKPOINTS_DIR\n",
    ")\n",
    "\n",
    "print(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancer l'entra√Ænement\n",
    "trainer.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'historique\n",
    "trainer.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le final\n",
    "final_model_path = os.path.join(MODELS_DIR, f\"{MODALITY}_model_final.pth\")\n",
    "trainer.save_checkpoint(final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Phase 6 : Inf√©rence (Optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le predictor\n",
    "predictor = Predictor(\n",
    "    model=model,\n",
    "    preprocessor=preprocessor,\n",
    "    device=DEVICE,\n",
    "    cube_size=CUBE_SIZE\n",
    ")\n",
    "\n",
    "# Charger le meilleur mod√®le\n",
    "# predictor.load_model(best_model_path)\n",
    "\n",
    "print(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'inf√©rence sur un nouveau volume\n",
    "# test_patient_path = os.path.join(SERIES_DIR, \"<SeriesInstanceUID>\")\n",
    "# prediction = predictor.predict_volume(test_patient_path, threshold=0.5)\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Pipeline Termin√©\n",
    "\n",
    "Vous avez ex√©cut√© le pipeline complet :\n",
    "- ‚úÖ Analyse exploratoire\n",
    "- ‚úÖ Cr√©ation du dataset\n",
    "- ‚úÖ Augmentation\n",
    "- ‚úÖ Entra√Ænement\n",
    "- ‚úÖ √âvaluation\n",
    "\n",
    "Les r√©sultats sont sauvegard√©s dans :\n",
    "- `results/processed/` : Datasets\n",
    "- `results/models/` : Mod√®les entra√Æn√©s\n",
    "- `results/checkpoints/` : Checkpoints d'entra√Ænement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
