{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import des classes et fonctions depuis src\n",
    "from src import (\n",
    "    SERIES_DIR, TRAIN_CSV, TRAIN_LOCALIZERS_CSV, OUTPUT_DIR,\n",
    "    print_config\n",
    ")\n",
    "from src.bricks import EDA, Preprocessor\n",
    "from src import show_middle_slices\n",
    "\n",
    "# Afficher la configuration\n",
    "print(\"=\"*60)\n",
    "print(\"Configuration de l'environnement\")\n",
    "print(\"=\"*60)\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration des Donn√©es DICOM\n",
    "\n",
    "Ce notebook utilise la classe **EDA** pour analyser les donn√©es locales de mani√®re syst√©matique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es\n",
    "df_train = pd.read_csv(TRAIN_CSV)\n",
    "df_localizers = pd.read_csv(TRAIN_LOCALIZERS_CSV)\n",
    "\n",
    "print(f\"üìä Donn√©es charg√©es:\")\n",
    "print(f\"  - S√©ries totales: {len(df_train)}\")\n",
    "print(f\"  - Localisateurs (avec an√©vrismes): {len(df_localizers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er l'analyseur EDA\n",
    "eda = EDA(df_train, df_localizers, SERIES_DIR)\n",
    "\n",
    "print(f\"‚úÖ Analyseur EDA cr√©√©\")\n",
    "print(eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Analyser les modalit√©s\n",
    "modality_counts = eda.analyze_modalities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£ Analyser la distribution des an√©vrismes\n",
    "aneurysm_counts = eda.analyze_aneurysm_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ Analyser les positions anatomiques\n",
    "position_counts = eda.analyze_positions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 4Ô∏è‚É£ Visualiser les distributions\n",
    "eda.plot_aneurysm_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 5Ô∏è‚É£ D√©tecter les s√©ries d√©fectueuses (optionnel - peut √™tre long)\n",
    "# Commentez cette cellule si vous avez beaucoup de s√©ries\n",
    "\n",
    "# defective_series = eda.detect_defective_series()\n",
    "print(\"‚ö†Ô∏è D√©tection des s√©ries d√©fectueuses comment√©e (peut √™tre long)\")\n",
    "print(\"D√©commentez la cellule pour l'ex√©cuter sur vos 20 s√©ries locales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 6Ô∏è‚É£ Analyser le nombre de slices\n",
    "slice_stats = eda.analyze_slice_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7Ô∏è‚É£ Rapport complet\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAPPORT COMPLET\")\n",
    "print(\"=\"*60)\n",
    "eda.generate_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualisation de S√©ries DICOM\n",
    "\n",
    "Utilisons le **Preprocessor** pour charger et visualiser des volumes 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un preprocessor\n",
    "preprocessor = Preprocessor(target_spacing=(0.4, 0.4, 0.4))\n",
    "\n",
    "print(\"‚úÖ Preprocessor cr√©√©\")\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lectionner une s√©rie au hasard parmi les locales\n",
    "import os\n",
    "import glob\n",
    "\n",
    "available_series = glob.glob(os.path.join(SERIES_DIR, '*'))\n",
    "\n",
    "if available_series:\n",
    "    # Prendre la premi√®re s√©rie disponible\n",
    "    example_series = available_series[0]\n",
    "    series_uid = os.path.basename(example_series)\n",
    "    \n",
    "    print(f\"üìÅ S√©rie s√©lectionn√©e: {series_uid}\")\n",
    "    print(f\"üìÇ Chemin: {example_series}\")\n",
    "    \n",
    "    # Compter les fichiers DICOM\n",
    "    dicom_files = glob.glob(os.path.join(example_series, '*.dcm'))\n",
    "    print(f\"üî¢ Nombre de slices: {len(dicom_files)}\")\n",
    "else:\n",
    "    print(\"‚ùå Aucune s√©rie DICOM trouv√©e dans:\", SERIES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger et pr√©processer le volume avec le Preprocessor\n",
    "if available_series:\n",
    "    print(\"üîÑ Chargement et preprocessing du volume...\")\n",
    "    volume = preprocessor.process_volume(example_series)\n",
    "    \n",
    "    print(f\"‚úÖ Volume pr√©process√©:\")\n",
    "    print(f\"  - Shape: {volume.shape}\")\n",
    "    print(f\"  - Min: {volume.min():.4f}\")\n",
    "    print(f\"  - Max: {volume.max():.4f}\")\n",
    "    print(f\"  - Mean: {volume.mean():.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Passez √† la cellule suivante\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les coupes du volume avec show_middle_slices\n",
    "if available_series:\n",
    "    print(\"üìä Visualisation des coupes centrales (axiale, coronale, sagittale):\")\n",
    "    show_middle_slices(volume)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aucun volume √† visualiser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualisation de Plusieurs S√©ries\n",
    "\n",
    "Visualisons les 3 premi√®res s√©ries disponibles localement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les 3 premi√®res s√©ries\n",
    "n_series_to_show = min(3, len(available_series))\n",
    "\n",
    "for i in range(n_series_to_show):\n",
    "    series_path = available_series[i]\n",
    "    series_uid = os.path.basename(series_path)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"S√©rie {i+1}/{n_series_to_show}: {series_uid}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Preprocessing\n",
    "        volume = preprocessor.process_volume(series_path)\n",
    "        print(f\"‚úÖ Volume shape: {volume.shape}\")\n",
    "        \n",
    "        # Visualisation\n",
    "        show_middle_slices(volume)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du traitement: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook a utilis√© les composants modulaires de `src/` :\n",
    "\n",
    "### ‚úÖ Classe EDA\n",
    "- Analyse des modalit√©s\n",
    "- Distribution des an√©vrismes\n",
    "- Positions anatomiques\n",
    "- D√©tection de s√©ries d√©fectueuses\n",
    "- Statistiques sur les slices\n",
    "\n",
    "### ‚úÖ Classe Preprocessor\n",
    "- Chargement DICOM\n",
    "- Resampling √† espacement cible\n",
    "- Cropping du fond\n",
    "- Normalisation\n",
    "\n",
    "### ‚úÖ Fonctions de Visualisation\n",
    "- `show_middle_slices()` pour visualiser les coupes 3D\n",
    "\n",
    "### üéØ Prochaines √©tapes\n",
    "\n",
    "Consultez le notebook [02_dataset_creation.ipynb](02_dataset_creation.ipynb) pour cr√©er un dataset d'entra√Ænement √† partir de ces donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Modality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MRA=df[df['Modality']=='MRA']\n",
    "df_CTA=df[df['Modality']=='CTA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MRA=df_MRA[['SeriesInstanceUID', 'PatientPath', 'NumSlices', 'Aneurysm Present']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MRA['Aneurysm Present'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Aneurysm Present'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MRA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MRA['NumSlices'].hist(bins=100)\n",
    "plt.xlabel(\"Nombre de slices\")\n",
    "plt.ylabel(\"Nombre de s√©ries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing des DICOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser SERIES_DIR\n",
    "series_path = SERIES_DIR\n",
    "patient_list = glob.glob(series_path+'/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer une s√©rie d'images DICOM en tableau 3D numpy\n",
    "def dicom_to_numpy(patient_path):\n",
    "    \n",
    "    dicom_files = sorted(glob.glob(patient_path+'/*.dcm'))\n",
    "    slices = [pydicom.dcmread(f) for f in dicom_files]\n",
    "    #tri des slices par instance number\n",
    "    slices.sort(key=lambda s: int(s.InstanceNumber))\n",
    "    \n",
    "    # On empile les pixel_array en un volume 3D NumPy (X,Y,Z)\n",
    "    target_shape = slices[0].pixel_array.shape\n",
    "    slices = [s for s in slices if s.pixel_array.shape == target_shape]\n",
    "    volume = np.stack([s.pixel_array for s in slices], axis=-1)\n",
    "\n",
    "    # R√©cup√©ration du spacing r√©el\n",
    "    pixel_spacing = slices[0].PixelSpacing\n",
    "    dx, dy = pixel_spacing\n",
    "    dz = getattr(slices[0], 'SliceThickness', 1.0)  # fallback si manquant\n",
    "    \n",
    "    return volume, (dx,dy,dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(volume, spacing):\n",
    "    dx, dy, dz = spacing\n",
    "    new_volume = zoom(volume, (dx/1, dy/1, dz/1), order=1)\n",
    "    return new_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(volume):\n",
    "    \"\"\"\n",
    "    Coupe le volume 3D pour ne garder que la r√©gion contenant du signal.\n",
    "    threshold : valeurs < threshold sont consid√©r√©es comme fond/noir.\n",
    "    \"\"\"\n",
    "    # On cr√©e un masque des voxels non nuls\n",
    "    mask = volume > 10\n",
    "    if not mask.any():\n",
    "        return volume  # rien √† couper\n",
    "    \n",
    "    # On r√©cup√®re les indices min/max pour chaque dimension\n",
    "    x_min, x_max = mask.any(axis=(1,2)).nonzero()[0][[0, -1]]\n",
    "    y_min, y_max = mask.any(axis=(0,2)).nonzero()[0][[0, -1]]\n",
    "    z_min, z_max = mask.any(axis=(0,1)).nonzero()[0][[0, -1]]\n",
    "    \n",
    "    # Crop\n",
    "    cropped = volume[x_min:x_max+1, y_min:y_max+1, z_min:z_max+1]\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(volume):\n",
    "    \"\"\"\n",
    "    Redimensionne le volume 3D √† la taille target_shape par interpolation lin√©aire.\n",
    "    \"\"\"\n",
    "    target_shape=(128,128,64)\n",
    "    factors = [t/s for t, s in zip(target_shape, volume.shape)]\n",
    "    resized_volume = zoom(volume, factors, order=1)\n",
    "    return resized_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliser un tableau numpy 3D\n",
    "def normalization(volume):\n",
    "    \"\"\"\n",
    "    Normalise un volume entre 0 et 1 (par patient).\n",
    "    \"\"\"\n",
    "    v_min, v_max = volume.min(), volume.max()\n",
    "    if v_max > v_min:  # √©viter la division par z√©ro\n",
    "        volume = (volume - v_min) / (v_max - v_min)\n",
    "    else:\n",
    "        volume = np.zeros_like(volume)\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(patient_path):\n",
    "    \n",
    "    volume,spacing = dicom_to_numpy(patient_path)\n",
    "    volume = resample(volume, spacing)\n",
    "    volume = crop(volume)\n",
    "    volume = resize(volume)\n",
    "    volume = normalization(volume)\n",
    "\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creer un Dataset PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AneurysmDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)  # ton DataFrame avec PatientPath et label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]                # r√©cup√®re la ligne correspondante\n",
    "        volume = preprocessing(row['PatientPath'])  # pipeline 3D complet\n",
    "        label = row['Aneurysm Present']        # 0 ou 1\n",
    "\n",
    "        # PyTorch attend (C, X, Y, Z) ‚Üí ajouter une dimension channel\n",
    "        volume = torch.tensor(volume, dtype=torch.float32).unsqueeze(0)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return volume, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_train = df.iloc[:100]  # 50 patients pour train\n",
    "subset_val = df.iloc[100:150] #10 patients pour validation\n",
    "\n",
    "train_dataset=AneurysmDataset(subset_train)\n",
    "val_dataset=AneurysmDataset(subset_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df_MRA, test_size=0.2, \n",
    "                                                random_state=42, \n",
    "                                                stratify=df_MRA['Aneurysm Present'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AneurysmDataset(df_train)\n",
    "val_dataset = AneurysmDataset(df_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 3D\n",
    "On est pas encore sur un resnet en fait mais sur un CNN 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResNet3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleResNet3D, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(16)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, stride=2, padding=1)  # downsample\n",
    "        self.bn2 = nn.BatchNorm3d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm3d(64)\n",
    "        \n",
    "        self.conv4 = nn.Conv3d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm3d(128)\n",
    "        \n",
    "        # Global average pooling ‚Üí r√©sum√© du volume\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d(1)  \n",
    "        \n",
    "        # Classification binaire\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        x = self.avgpool(x)  # (B, 128, 1, 1, 1)\n",
    "        x = torch.flatten(x, 1)  # (B, 128)\n",
    "        x = self.fc(x)  # (B, 1)\n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = SimpleResNet3D().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (volumes, labels) in enumerate(tqdm(train_loader)):\n",
    "        volumes = volumes.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True).unsqueeze(1).float()  # (B,1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(volumes)  # d√©j√† sigmoid\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * volumes.size(0)\n",
    "\n",
    "        # Sauvegarde toutes les 10 it√©rations (par ex.)\n",
    "        if batch_idx % 10 == 0:\n",
    "            checkpoint_path = f\"checkpoint_epoch{epoch}_batch{batch_idx}.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'batch_idx': batch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss.item(),\n",
    "            }, checkpoint_path)\n",
    "        \n",
    "    return running_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for volumes, labels in tqdm(val_loader):\n",
    "            volumes = volumes.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True).unsqueeze(1).float()\n",
    "            \n",
    "            outputs = model(volumes)  # d√©j√† sigmo√Ød\n",
    "            preds = outputs  # proba\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')  # si une seule classe dans val\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un mod√®le sauvegard√© localement\n",
    "# Note: Adapter le chemin vers votre mod√®le local\n",
    "# state_dict = torch.load(\n",
    "#     \"results/models/best_model_epoch3.pth\",\n",
    "#     map_location=\"cuda\"\n",
    "# )\n",
    "# model.load_state_dict(state_dict)\n",
    "\n",
    "print(\"‚ö†Ô∏è Cette cellule charge un mod√®le pr√©-entra√Æn√© depuis Kaggle.\")\n",
    "print(\"Commentez cette cellule si vous n'avez pas encore de mod√®le local.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester avec un train et val loaders r√©duits\n",
    "num_epochs = 4\n",
    "\n",
    "best_auc = 0.5605  # pour sauvegarder le meilleur mod√®le\n",
    "\n",
    "for epoch in range(3,num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_auc = evaluate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        torch.save(model.state_dict(), f\"best_model_epoch{epoch+1}.pth\")\n",
    "        print(f\"--> Mod√®le sauvegard√© √† l'epoch {epoch+1} avec AUC {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/default/1/checkpoint_epoch1_batch140.pth\", map_location=device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "start_epoch = checkpoint['epoch']\n",
    "start_batch = checkpoint['batch_idx'] + 1  # on repart juste apr√®s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester avec un train et val loaders r√©duits\n",
    "num_epochs = 3\n",
    "\n",
    "best_auc = 0.0  # pour sauvegarder le meilleur mod√®le\n",
    "\n",
    "for epoch in range(2,num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_auc = evaluate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        torch.save(model.state_dict(), f\"best_model_epoch{epoch+1}.pth\")\n",
    "        print(f\"--> Mod√®le sauvegard√© √† l'epoch {epoch+1} avec AUC {val_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13441085,
     "sourceId": 99552,
     "sourceType": "competition"
    },
    {
     "datasetId": 8089774,
     "sourceId": 12795449,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 430741,
     "modelInstanceId": 412987,
     "sourceId": 527737,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 431788,
     "modelInstanceId": 414048,
     "sourceId": 529324,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
