{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement du Modèle U-Net 3D\n",
    "\n",
    "Ce notebook entraîne un modèle U-Net 3D pour la détection d'anévrismes.\n",
    "\n",
    "**Étapes** :\n",
    "1. Chargement du dataset créé\n",
    "2. Split train/val/test\n",
    "3. Création des DataLoaders PyTorch\n",
    "4. Configuration du modèle et de l'entraînement\n",
    "5. Entraînement avec Trainer\n",
    "6. Évaluation et visualisation des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src import PROCESSED_DIR, MODELS_DIR\n",
    "from src.models import UNet3DClassifier\n",
    "from src.bricks import Trainer\n",
    "\n",
    "# Seed pour reproductibilité\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement du Dataset\n",
    "\n",
    "Chargement du dataset CTA créé dans le notebook 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Charger le dataset CTA\n",
    "dataset_path = os.path.join(PROCESSED_DIR, \"cta_dataset.npz\")\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    loaded = np.load(dataset_path, allow_pickle=True)\n",
    "    data = {key: loaded[key].item() for key in loaded.files}\n",
    "    print(f\"Dataset chargé: {len(data)} patients\")\n",
    "    print(f\"Exemple - patient_0 a {len(data['patient_0']['cubes'])} cubes\")\n",
    "else:\n",
    "    print(f\"Dataset non trouvé : {dataset_path}\")\n",
    "    print(\"Exécutez d'abord le notebook 02_dataset_creation.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split Train/Val/Test\n",
    "\n",
    "Division des données par patient (70% train, 15% val, 15% test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split par patient\n",
    "all_patients = list(data.keys())\n",
    "\n",
    "train_patients, temp_patients = train_test_split(\n",
    "    all_patients, test_size=0.3, random_state=SEED\n",
    ")\n",
    "val_patients, test_patients = train_test_split(\n",
    "    temp_patients, test_size=0.5, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"{len(train_patients)} train, {len(val_patients)} val, {len(test_patients)} test\")\n",
    "\n",
    "train_data = {k: data[k] for k in train_patients}\n",
    "val_data = {k: data[k] for k in val_patients}\n",
    "test_data = {k: data[k] for k in test_patients}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset PyTorch\n",
    "\n",
    "Classe Dataset pour charger les cubes en batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CubesDataset(Dataset):\n",
    "    \"\"\"Dataset PyTorch pour cubes 3D.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dict, transform=None):\n",
    "        self.transform = transform\n",
    "        self.cubes = np.concatenate([d['cubes'] for d in data_dict.values()], axis=0)\n",
    "        self.positions = np.concatenate([d['positions'] for d in data_dict.values()], axis=0)\n",
    "        self.labels = np.concatenate([d['labels'] for d in data_dict.values()], axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cubes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cube = self.cubes[idx]\n",
    "        label = self.labels[idx]\n",
    "        position = self.positions[idx]\n",
    "        \n",
    "        # Convertir en tenseur PyTorch\n",
    "        cube = torch.tensor(cube, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Concaténer position (13) et label (1) -> shape (14,)\n",
    "        y = np.concatenate([position, [label]], axis=0)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        return cube, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DataLoaders\n",
    "\n",
    "Création des DataLoaders pour l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "train_dataset = CubesDataset(train_data)\n",
    "val_dataset = CubesDataset(val_data)\n",
    "test_dataset = CubesDataset(test_data)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Vérifier les shapes\n",
    "for cube, y in train_loader:\n",
    "    print(f\"Cube shape: {cube.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Function\n",
    "\n",
    "Loss combinée : BCE pour les positions + BCE pour le label binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_loss(pred, target, alpha=0.1):\n",
    "    \"\"\"Loss combinée pour positions + label.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : torch.Tensor\n",
    "        Prédictions (B, 14)\n",
    "    target : torch.Tensor\n",
    "        Targets (B, 14)\n",
    "    alpha : float\n",
    "        Poids pour la loss de position\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Loss combinée\n",
    "    \"\"\"\n",
    "    pos_pred = torch.sigmoid(pred[:, :13])\n",
    "    pos_target = target[:, :13]\n",
    "    label_pred = pred[:, 13:]\n",
    "    label_target = target[:, 13:]\n",
    "    \n",
    "    loss_pos = F.binary_cross_entropy(pos_pred, pos_target)\n",
    "    loss_label = F.binary_cross_entropy_with_logits(label_pred, label_target)\n",
    "    \n",
    "    return alpha * loss_pos + loss_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration du Modèle\n",
    "\n",
    "Utilisation de UNet3DClassifier depuis src.models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Modèle\n",
    "model = UNet3DClassifier(in_ch=1, base_ch=32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(f\"Modèle: {model}\")\n",
    "print(f\"Nombre de paramètres: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entraînement avec Trainer\n",
    "\n",
    "Utilisation de la classe Trainer depuis src.bricks pour l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    criterion=combined_loss,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    checkpoint_dir=MODELS_DIR\n",
    ")\n",
    "\n",
    "# Entraîner\n",
    "trainer.fit(train_loader, val_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualisation des Résultats\n",
    "\n",
    "Affichage des courbes de loss et accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'historique\n",
    "trainer.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sauvegarde du Modèle\n",
    "\n",
    "Sauvegarde du modèle entraîné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle final\n",
    "import os\n",
    "final_model_path = os.path.join(MODELS_DIR, \"unet3d_cta_final.pth\")\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Modèle final sauvegardé : {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Évaluation sur Test Set\n",
    "\n",
    "Évaluation finale sur le test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer sur test set\n",
    "test_loss, test_acc = trainer.validate(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13762876,
     "sourceId": 99552,
     "sourceType": "competition"
    },
    {
     "datasetId": 8322407,
     "sourceId": 13136624,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 455943,
     "modelInstanceId": 439382,
     "sourceId": 587738,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
